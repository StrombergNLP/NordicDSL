\section{Visualization}
In this section we will try to develop some intuition about the dataset by making different visualizations.

\subsection{Character distributions}
One obvious thing to look at is how frequent each character appears in each language.
In Figure \ref{chardist}\footnote{Some of the characters miss Diacritics i could not find a solution for matplotlib to handle there characters.} I have plotted the frequency by which each character appears for each language in the corpus, in Figure \ref{chardistnorm} i have normalized the distribution by dividing each occurrence of a character with the total number of times that character appear in the dataset.\\
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/chardist}
        \caption{Character frequencies}
        \label{chardist}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/chardistnorm}
        \caption{Normalized character frequencies}
        \label{chardistnorm}
    \end{subfigure}
    \caption{Frequency of each character in each language. The character are ordered along the x-axis according to the frequency by which they appear in the dataset. }
    %\label{chardist}
\end{figure}

Interestingly the most frequent character {\tt e} is much less frequent in Faroese and Icelandic than in the other languages. Not surprisingly the character  {\tt ð} only appears in Faroese and Icelandic while it is much more frequent in Icelandic. The character {\tt þ} is exclusive to Icelandic and  {\tt ö} almost only appears in Swedish. We could potentially use the information about the relative frequencies of characters in the models.


\subsection{Principal Component analysis and t-SNE}
To gain additional insight on how the different word embedding capture important information about each of the language classes I thought it would be interesting to try and visualize the embeddings using two different techniques for dimensionality reduction.\\

No matter which way we choose to extract the feature vectors they belong to a high dimensional feature space and in order to do visualization we need to project the feature vectors down to 2d space.\\

To do this I have implemented two different methods: Principal Component Analysis (PCA) which i will compare with T-distributed Stochastic Neighbor Embedding (t-SNE).
Here we will begin with a brief explanation of the two techniques and proceed with an analysis of the results.

\paragraph{Pricipal Component Analysis}

The first step is to calculate the covariance matrix of the dataset.
The components of the covariance matrix is given by

\begin{align}
K_{X_i,X_j} = E[(X_i - \mu_i )(X_j -  \mu_j)]
\end{align}

where $X_{i}$ is the $i$th component of the feature vector and $\mu_{i}$ is the mean of that component.

In matrix form we can thus write the covariance matrix as
\begin{align}
K(\mathbf{x},\mathbf{z}) =
\begin{bmatrix}
    \text{cov}(x_1,z_1) &  \dots  & \text{cov}(x_1,z_n) \\
    \vdots & \ddots     & \vdots \\
    \text{cov}(x_n,z_1) & \dots  & \text{cov}(x_n,z_n) \\
\end{bmatrix}
\end{align}
The next step is to calculate the eigenvectors and eigenvalues of the covariance matrix by solving the eigenvalue equation.
\begin{align}
\det (K v-\lambda v) = 0
\end{align}
The eigenvalues are the variances along the direction of the eigenvectors or "Principal Components". To project our dataset onto 2D space we select the two eigenvectors largest associated eigenvalue and project our dataset onto this subspace.\\

In Figure \ref{pca} we see the result of running the PCA algorithm on the wikipedia dataset where we have used character level bigrams as features as well as the cbow and skipgram models from FastTest.\\


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/pcachar2}
        \caption{Character bigram}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/pcacbow1}
        \caption{Fasttext cbow}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/pcaskipgram1}
        \caption{Fasttext skipgram}
    \end{subfigure}
    \caption{Dimensionality reduction using PCA}
    \label{pca}
\end{figure}

In the figure for encoding with character level bi-grams the PCA algorithm resulted in two elongated clusters. Without giving any prior information about the language of each sentences the PCA is apparently able to discriminate between Danish, Swedish, Nynorsk and Bokmål on one side and Faroese and Icelandic on the other since the majority of the sentences in each language belong to either of these two clusters. With the FastText implementations we observe three clusters.\\

For both cbow and skipgram we see a distinct cluster of Sweedish sentences. When comparing the two FastText models we see that the t-SNE algorithm with skipgrams seems to be able to separate the Faroese and Icelandic data points to a high d ecree compared with the cbow model. Also for the cluster identified with the sentences with Danish, Bokmål and Nynorsk the skipgram models seem seem to give a better separation, however to a lesser degree than with the two former languages.

\paragraph{t-SNE}

The T-distributed Stochastic Neighbor Embedding method was first proposed in 2008 in the paper "Visualizing Data using t-SNE"\cite{tsne}.

In the paper the authors explain the theory behind the algorithm which I  will make a brief summary of here.

Suppose you pick a data point $x_i$, then the probability of picking another data point $x_j$ as a neighbor to $x_i$ is given by
\begin{align}
p_{ji}= \frac{\exp (|| x_i - x_j ||^2/2\sigma_i^2 )}{\sum_{k\neq i} \exp (|| x_i - x_k ||^2/2\sigma_i^2 )}
\end{align}

Now having this probability distribution the goal is to find the low-dimensional mapping of the data points $x_i$ which we denote $y_i$ follow a similar distribution. To solve what is referred to as the "crowding problem" the t-SNE algorithm uses the "Student t-distribution" which is given by
\begin{align}
q_{ij}= \frac{ (1+|| y_i - y_j ||^2 )^{-1}}{\sum_{k\neq l} (1+|| y_k - y_l ||^2 )^{-1}}
\end{align}
Now finally for optimizing this distribution is done by using gradient decent on the "Kullback-Leibler divergence" which is given by
\begin{align}
\frac{\delta C}{\delta y_i}= 4 \sum_j (p_{ij} - q_{ij})(y_i-y_j)(1+ || y_i - y_j ||^2  )^{-1}
\end{align}

The result from running the t-SNE algorithm on the wikipedia dataset can be seen in Figure \ref{tsne}. As was the case with the PCA algorithm it appears that the encoding with FastText seem to capture the most relevant information to discriminate between the languages, especially the skipgram mode seems to do a good job in capturing relevant information.\\

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/tsnechar2}
        \caption{Character bigram}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/tsnecbow1}
        \caption{Fasttext cbow}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{figs/tsneskipgram1}
        \caption{Fasttext skipgram}
    \end{subfigure}
    \caption{Dimensionality reduction using t-SNE}
    \label{tsne}
\end{figure}


Here we recover some interesting information about the similarity of the languages. The data points in Bokmål lies in between those in Danish and Nynorsk while Icelandic and Faroese have their own two clusters which are separated from the three former languages. \\

This is in good agreement with what we already know about the languages. Interestingly the Swedish data points and quite scattered and the t-SNE is not able to make a coherent Swedish cluster.\\

This does not however mean that the Swedish datapoint are not close in the original space. Some care is needed when interpreting the plot since t-SNE groups together data points such that neighboring points in the input space will tend to be neighbors in the low dimensional space.\\

If points are separated in input space, t-SNE would like to separate them in the low dimensional space however it does not care how far they are separated. So clusters that are far away in the low dimensional space are not necessarily far away in the input space.

% Unlike PCA, axes in the low dimensional space don't have a particular meaning. In fact, one could arbitrarily rotate the low dimensional points and the t-SNE cost function wouldn't change. Furthermore, t-SNE doesn't construct explicit mappings relating the high dimensional and low dimensional spaces.
%
% Rather, the relevant information is in the relative distances between low dimensional points. t-SNE captures structure in the sense that neighboring points in the input space will tend to be neighbors in the low dimensional space.
%
% But, some care is needed because larger distances can't necessarily be interpreted. If points are separated in input space, t-SNE would like to separate them in the low dimensional space. But, it doesn't care how far they are (unlike PCA, MDS, or isomap, for example). Another issue is that t-SNE sometimes breaks continuous segments of data into pieces and artificially separates them, particularly at low perplexity settings. See here for a good example. t-SNE is framed as a visualization tool rather than a pre-processing or analysis tool, and doing things like clustering in the low dimensional space can be dangerous because of these issues. The upshot is that distorting distances sometimes lets t-SNE produce good 2/3d visualizations of data that are intrinsically higher dimensional.
%
% One way that t-SNE visualizations can be useful is by combining them with external information. This can reveal patterns in the data that we may not have been aware of. For example, the t-SNE papers show visualizations of the MNIST dataset (images of handwritten digits). Images are clustered according to the digit they represent--which we already knew, of course. But, looking within a cluster, similar images tend to be grouped together (for example, images of the digit '1' that are slanted to the left vs. right). And, points that appear in the 'wrong' cluster are sometimes actually mislabelled in the original dataset, or ambiguously written (e.g. something between '4'and a '9').
