\section{Conclusion}
This paper presented research on the difficult task of distinguishing similar languages applied for the first time to the Scandinavian context. To address this, we first created a datasets.  We compared four different "classical" models: K nearest Neighbors, Logistic regression, Naive Bayes and a linear support vector machines with two neural network architectures: Multilayer perceptron and a convolutional neural network.\\

The two best performing models, FastText supervised and CNN, saw low performance when going off-domain. Using character n-grams as features instead of words increased the performance for the FastText supervised classifier. By also training FastText on the Tatoeba dataset as well as the Wikipedia dataset resulted in an additional increase in performance.\\

The data from this task will be released openly, as well as a CodaLab instance for others to participate directly.

