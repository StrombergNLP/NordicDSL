\section{Experiments}\label{sec:results}

In the following in Sec.~\ref{sec:data} we first describe the database, which we used to create different tensor models proposed in Sec.~\ref{sec:tensormodel}. 
Based on these, we present substructures which we found in the auxiliary latent space in Sec.~\ref{sec:subspace}. 
A qualitative comparison of the reconstruction quality of the two different versions of the tensor model is presented in  Sec.~\ref{sec:extmodel}. 
We proceed to quantitatively evaluate the effect of varying regularisation parameters on approximation and transfer in Sec.~\ref{sec:validation}, and then apply these findings by a qualitative results presented in Sec.~\ref{sec:approx}. 

\subsection{Facial Expression Database} \label{sec:data}
In this work, we use the Binghamton University 3D Facial Expression database (BU-3DFE)\cite{bu3dfe}. 
It contains 3D face scans and images of 100 persons (56 female and 44  male), with varying age (18-70 years) and diverse ethnic/races. 
Each subject was asked to perform the six basis emotions: anger, disgust, happiness, fear, sadness, and surprise, each with four levels of increasing expression strength. 
Additionally, for each participant the neutral face was recorded.
Hence, a total of 25 facial expressions is provided, recorded from two different rotations, %($\pm \pi/4$ from frontal)
i.e. left and right rotations, from which an additional morphed frontal image is provided.
% Maybe something this these?
% \input{tex/fig/Fig3/fig3.tex}

%\subsection{Subspace Analysis}
\subsection{Subspace Analysis of the Auxiliary Latent Space}
\label{sec:subspace}
% Sec.~\ref{sec:latent_space_embedding} describes how the face images of the BU3D-FE database can be embedded into the latent space of the StyleGAN1 and 2. The resulting latent vectors are then ordered into a data tensor, which is factorised by a HOSVD, as described in  Eq.~\ref{eq:matrix_hosvd}, leading to three different subspaces, represented by the matrices $\matr{U}_k$. 
The BU-3DFE database offers 7500 face images, see Sec.~\ref{sec:data}, %of 100 persons in 25 expressions, i.e. the 6 basic emotions in 4 intensity levels and neural, see Sec.~\ref{sec:data}.
which we embed into the latent space of StyleGAN1 and 2, as described in Sec.~\ref{sec:stylegan}, hence for each face image one latent vector is obtained. 
The resulting latent vectors are ordered into a 4way data tensor $T\in\R^{N\times P\times E \times R}$. %where $N$ is the number of elements of one latent vector, $P$ refers to 100 persons and $E$ are the 25 expressions. 
The 4-way data tensor is then factorised by the HOSVD, as described in \eqref{eq:matrix_hosvd} leading to four different subspaces %spanned by the columns of the associated orthogonal matrices
$\matr{U}_k$, $k=1,\ldots,4$. % reflecting the different dimensions of the data tensor. 
The first three dimensions of the expression subspace spanned by the columns of $\matr{U}_3$ are visualised in Fig.~\ref{fig:apathy_vs_neutral}\subref{subfig:subspaces}. 
It can be seen that the for each emotion, the four points of different varying expression strengths form linear trajectories, which intersect a new expression, which does not equal the neutral expression, nor the mean face. 
These three expressions were synthesised for the mean person and are illustrated in Fig.~\ref{fig:apathy_vs_neutral}. 
It can be seen that the mouth in the mean and the neutral expression is open and looks rather happy, whereas the mouth is closed in the newly synthesised  expression, hence we labelled it as \emph{apathetic} as in \cite{apathy,grasshof2020}. 
This confirms the findings from \cite{apathy,grasshof2020} based on the same database and others. 
Similarly, we utilise the structure found in the subspace by employing reasonable constraints for the optimisation procedure and hence improve the results for for new inputs, which we show in Sec.~\ref{sec:approx}. 

%based on data of 3D faces from the same database and an additional 2D database.
% To visualise the differences between the three expressions, each of them was synthesised and is illustrated in Fig.~\ref{fig:apathy_vs_neutral} on the mean person. It can be seen that the mouth in the mean and the neutral expression is open and looks rather happy, whereas the mouth is closed in the apathetic expression. 
%This shows that there is a structure in the subspaces, which we enforce to stick to by constrained optimisation described in Sec.~\ref{sec:optimisation} for new inputs. 
%  labelled as \emph{apathetic}. 
%%%%%
%It can be seen that the four points describing different expression strengths for each emotion (visualised in different colors) form trajectories and that these intersect in a new expression, which does not equal the neutral expression, nor the mean face, labelled as \emph{apathetic}. 
% % % FIGURE 5
\input{fig/Fig5/fig5.tex}


%\subsection{Vectorised vs. Extended Model}
\subsection{Vectorised vs. Stacked Style-Separated Model Model}
\label{sec:extmodel}

On the basis of the auxiliary latent space, we proposed to build two different versions of tensor models in Sec.~\ref{sec:tensormodel}. Both are based on a factorisation of different data tensors: 

(1) The \emph{vectorised model} which vectorises each latent code of one image and then orders them into a data tensor $T\in\R^{N\times P\times E\times R}$, and 

(2) the \emph{stacked style-separated model} $T_\text{style}\in\R^{S\times L\times P\times E\times R}$ which considers the $S=18$ styles of StyleGAN separately, and hence is based on $S$ style-specific tensors $T_s\in\R^{L\times P\times E\times R}$, $s=1,\ldots, S$, see Sec.~\ref{sec:extended_model}. 

In this section we compare the results obtained by the two models, each using the ALS procedure \eqref{eq:als} for optimisation. 
%we proposed to either build one tensor model $T$ $T^{(i)}$
%different models were proposed. 
%Fig.~\ref{fig:vec_vs_extended_model}
The results are illustrated in Fig.~\ref{fig:fig7}. It can be seen that the input images shown in Fig.~\ref{fig:fig7}\subref{subfig:reference}, are visually matched better by the \emph{stacked style-separated model}, presented in Fig.~\ref{fig:fig7}\subref{subfig:extended}, as compared to the results of the \emph{vectorised model} shown in Fig.~\ref{fig:fig7}\subref{subfig:vectorised}. This observation holds for test data samples of BU-3DFE shown in the top row, as well as for arbitrary examples shown in 2nd and 3rd row. 
Therefore, we conclude that the proposed adaptation considering the separate styles, improves the performance. 

\input{fig/Fig7/fig7.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Validation of Regularisation Parameters}
\label{sec:validation}

The optimisation problem defined in \eqref{eq:lagrangian} contains two hyper parameters: $\lambda_1$ and  $\lambda_2$, which must be manually set.
%\eqref{eq:optimisation_problem} 
%The proposed optimisation procedures contain hyper parameters, which need to be manually set. 
In this section we investigate how they influence the quality of the results using the previously defined \emph{vectorised model} on the basis of the standardised latent codes, i.e. $T\in\R^{N\times P\times E\times R}$ in \eqref{eq:matrix_model}. 
%, based on the \eqref{eq:lagrangian}. 

First, we separate the data tensor into  training, validation and test set by a randomised 90/5/5 over the $P=100$ person identities. As a result, the validation set has a total of $5E R=5\cdot25\cdot3=375$ samples. 
We build one tensor model based on the training set. 

%intro
Then for each sample of the validation set $\vec{y}_i\in\mathcal{W}$ we estimate the parameter tensor $\widehat{Q}_i$ by ALS in \eqref{eq:als}, which estimates the  parameter tensor  $\widehat{Q} = \widehat{\vec{q}}_\text{person} \otimes \widehat{\vec{q}}_\text{expression} \otimes \widehat{\vec{q}}_\text{rotation}$. The input sample is then approximated by $\widehat{\vec{y}}_i = \tau(\widehat{Q}_i)$ as in \eqref{eq:taugam}. 
%reconstruction
We define the approximation error between true $\vec{y}$ and estimated auxiliary latent code $\widehat{\vec{y}}_i$ as $\epsilon_\text{approx} =\epsilon(\widehat{\vec{y}},\vec{y})$ where the error metric is defined as
$\epsilon(\widehat{\vec{y}},\vec{y}) \equiv  \norm{\widehat{\vec{y}}-\vec{y}}^2$. 
%expression transfer
Additionally, we evaluate the transfer errors resulting from exchanging estimated parameters $\hat{\vec{q}}_k$ by known values $\tilde{\vec{q}}_k$. Hence using $\widetilde{\vec{y}}_\text{expr} \equiv \tau(\widehat{\vec{q}}_{\text{person}} \otimes \widetilde{\vec{q}}_{\text{expr}}\otimes \widehat{\vec{q}}_{\text{rot}})$ give rise to an expression transfer error which we define as $\epsilon_\text{expr}\equiv\epsilon(\widetilde{\vec{y}}_\text{expr},\vec{y})$. 
%rotation transfer
Analogously, we define a rotation transfer error $\epsilon_\text{rot} \equiv \epsilon(\widetilde{\vec{y}}_\text{rot},\vec{y})$ by calculating error arising from only changing the rotation part of the parameter tensor: $\widetilde{\vec{y}}_\text{rot} \equiv \tau(\widehat{\vec{q}}_{\text{person}}\otimes\widehat{\vec{q}}_{\text{expr}}\otimes\widetilde{\vec{q}}_{\text{rot}})$.

The three error metrics $\epsilon_\text{approx}$ $\epsilon_\text{expr}$, and $\epsilon_\text{rot}$ are calculated, for each sample, with varying hyper parameter values $\lambda_1$ and $\lambda_2$. Then the mean over all the sample-wise errors is computed, leading to $\overline{\epsilon}_{\cdot}$. 
The results are illustrated in Fig.~\ref{fig:hyper_param_errors}, where from top to bottom the mean errors are presented for: approximation error, expression transfer, and rotation transfer. 
In general it can be seen that the approximation error does not fluctuate as much as the other two error metrics. 
Fig.~\ref{fig:hyper_param_errors}\subref{subfig:l1penalty} indicates that high values of $\lambda_1$ should be chosen for rotation transfer, while for expression transfer $\lambda_1\approx 1$ seems to be a reasonable choice. 
Fig.~\ref{fig:hyper_param_errors}\subref{subfig:l2penalty} reveals that for $\lambda_2 \approx 1$ all error metrics are small, and hence this interval is a good choice for multiple usages. 
In the following section we use these findings by selecting specific parameter weights, and then perform expression and rotation transfer on samples the test set. 
\input{fig/Fig8/fig8_overlay.tex}
\subsection{Regularisation and Parameter Transfer}
\label{sec:approx}
In the previous section Sec.~\ref{sec:validation} quantitative results on expression and rotation transfer were presented, with respect to varying hyper parameters. 
%Here we pr
% In this section we show that the tensor model proposed in Sec.~\ref{sec:tensormodel} can be used to approximate unseen samples by employing the optimisation procedures described in Sec.~\ref{sec:optimisation}. 
In this section we estimate the parameters $\widehat{Q}$ for an unknown image $\vec{x}$ of the test set. We can retrieve an image from the estimated parameters by applying the composed transformation $\vec{x}\approx\widehat{\vec{x}}= g(\tau(\widehat{Q}))$. 
Additionally, we perform expression and rotation transfer by replacing one of the three estimated parameter vectors by known values, as described before. 
We do this for the regularised model, i.e. $\lambda_1>0,~\lambda_2>0$, and the non-regularised model, by choosing $\lambda_1=\lambda_2=0$. 
In Fig.~\ref{fig:fig9} the top row shows how well the ground truth (GT) presented in Fig.~\ref{fig:fig9}(a) can be approximated by Fig.~\ref{fig:fig9}(b) the non-regularised solution and Fig.~\ref{fig:fig9}(c) the regularised solution. When comparing them it seems that the non-regularised solution matches the GT slightly better. 
This statement also holds for the expression transfer shown in Fig.~\ref{fig:fig9}(b). 
However comparing the rotation transfer Fig.~\ref{fig:fig9}(c) shows that the regularised solution (bottom row) clearly outperforms the non-regularised solution (top row). Because in the non-regularised solution the resulting image is not recognisable as a face anymore at all, while the regularised solution is not deformed and the rotation of the depicted faces conform to the GT.
\input{fig/Fig9/fig9.tex}


% \begin{align}
%\frac{1}{N}\norm{\vec{y}-\vec{y}}^2   
% \epsilon_\text{approx} &=\epsilon(\widehat{\vec{y}},\vec{y}),\\
% \intertext{where the error metric is defined as}
% \epsilon(\widehat{\vec{y}},\vec{y}) &\equiv  \norm{\widehat{\vec{y}}-\vec{y}}^2. %\equiv \tensor{R}{_{\mu}} \tensor{R}{^{\mu}} 
%\intertext{is the error metric.}
%\vec{\vec{y}}&= f(\widehat{\vec{q}}_\text{person},\widehat{\vec{q}}_\text{expression},\widehat{\vec{q}}_\text{rotation})\\
% \end{align}
%is the approximated face image. 
%$\frac{1}{N}\norm{\vec{y}-\vec{y}}^2$ 
%$\hat{\vec{q}}_i \to \tilde{\vec{q}}_i$, i.e. for 
% \begin{align}
% \epsilon_\text{expr}&\equiv\epsilon(\widetilde{\vec{y}}_\text{expr},\vec{y}),\\
% %\widetilde{\vec{y}}_\text{expr} &\equiv \tau(\widehat{\vec{q}}_{\text{person}},\widetilde{\vec{q}}_{\text{expr}},\widehat{\vec{q}}_{\text{rot}}), \\
% \intertext{and with $\widetilde{\vec{y}}_\text{rot} \equiv \tau(\widehat{\vec{q}}_{\text{person}},\widehat{\vec{q}}_{\text{expr}},\widetilde{\vec{q}}_{\text{rot}})$ the rotation transfer error as}
% \epsilon_\text{rot}&\equiv\epsilon(\widetilde{\vec{y}}_\text{rot},\vec{y}).%\\
% %\widetilde{\vec{y}}_\text{rot} &\equiv \tau(\widehat{\vec{q}}_{\text{person}},\widehat{\vec{q}}_{\text{expr}},\widetilde{\vec{q}}_{\text{rot}}).
% \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%we present qualitative results on the basis of the test set.
%, such that for each hyper parameter value exactly one value for each error metric remains, which can then be represented by lines. 
% In Fig.~\ref{fig:hyper_param_errors}\subref{subfig:l1penalty} the $x$-axis refers to the hyper parameter $\lambda_1$, which weights the L1 penalty, while in Fig.~\ref{fig:hyper_param_errors}\subref{subfig:l2penalty} the $x$-axis refers to $\lambda_2$, which weights the L2 penalty. 

%
% \begin{align}
% P_\text{expr,transfer} = (\widehat{\vec{q}}_{\text{person}}\otimes\widetilde{\vec{q}}_{\text{expr}}\otimes\widehat{\vec{q}}_{\text{rot}})
% \end{align}
% \begin{align}
%     \widetilde{Y}_\text{e} = S(\widehat{\vec{q}}_{\text{person}}\otimes\widetilde{\vec{q}}_{\text{expr}}\otimes\widehat{\vec{q}}_{\text{rot}})
% \end{align}
%
