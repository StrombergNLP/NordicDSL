\section{Conclusions}
\label{sec:conclusion}

%+ tensor model in auxiliary latent space
%+ embedding of BU3DFE ()<maybe mention the compotational cost and resnet to decrease that embedding speed approx  (7500 data points) (2000 epochs)  (8 it/s ) =520hrs /21 days on tesla P100 (colab)

%+ previous findings confirmed: apathy mode 
%+ semantic subspaces
%- ?gradient descent and 
% Einstein Notation? could be its own point since we change the formalism non trivially when we introduce the combined rank 1 parameter tensor
%+ applications: approximation, expression and rotation transfer.

%Summary 
In this work, we have proposed $\tau$GAN a tensor based model for the auxiliary latent space of the StyleGAN. 
We first embedded the face database BU3DFE into the latent space. The resulting latent codes were stored into a multi dimensional  array or the data tensor, whose factorisation by HOSVD then yields semantically meaningful subspaces. We demonstrate the applicability approach by producing intuitive parametrisation for semantic face editing. 
%
%subspace (apathy)
Specifically, we were able to generalise the earlier result \cite{apathy} from feature-based face analysis:  %that suggested an alternative origin for the expressions
the expression subspace of the latent space namely has the structure where the expression trajectories meet in 
a specific emotionless face, the \emph{apathetic} expression, which is different from the mean or neutral face and the face. 
%
We evaluated our approach by quantitatively and qualitatively, and compared different versions of the proposed tensor models on the basis of approximation of unseen samples, and demonstrated stability in transfer of expression and rotation. From the results, we conclude that the proposed approach is powerful way for characterising and parameterising the latent spaces of StyleGAN.
%
%%%%%%%% Future work: keep it super short
In future, we intend to release a database by drawing samples from our auxiliary latent space. 

%\paragraph{Summary}

%\paragraph{Future work}

% \paragraph{Bigger and Better ResNet}
% Embedding a full data set of the scale of BU3D-FE into latent space is a very computationally intensive task. The estimated embedding time with 8it/s and for configuration B is 390 GPU hours on a single Tesla P100. The training of a larger (and hopefully better) ResNet would speed up the process by estimating better initial conditions for the VVG optimization. This would lower "the barrier of entry" for embedding data sets into the StyleGAN latent space.

% \paragraph{Time series data}
% Now, assuming we are able to estimate model parameters for an arbitrary reference image. The next logical step would be to estimate parameters for a time series and see how the parameters evolve. Each face in the tensor model is essentially a linear combination of the subspace basis vectors. We could model the time dependency of the expansion coefficients with Fourier analysis.

% \paragraph{Masking as post processing}
% In the configuration $\alpha$ run of the BU3D-FE latent space embedding we used the masking option from the code by \cite{pbaylies}. Having the unmasked latent vectors, it should be possible to add the mask as a post embedding step. This could be a way to augment the data. 

% \paragraph{Missing data}
% It is possible to define a generalized HOSVD which allows for missing data. This has been investigated in \cite{face2005}. It would be very interesting to implement a generalized model which allows for missing data, as this would make it possible to construct a tensor model on a less structured data set than the BU3D-FE database.  

% \paragraph{Creation of a synthetic data set based on the tensor model}
% We cannot release the tensor model since it is an approximation to the BU3D-FE data set which is under  Copyright by the authors. Our model can however generate synthetic faces with known parameters which do not exist in the BU3D-FE database. In principle we could generate a new synthetic data set, centered around the StyleGAN null\footnote{Mean face of StyleGAN, which ever version.} face, and then construct another tensor model on the synthetic data. 

% % \paragraph{Direct model embedding}
% % Another idea is to extimate the  model parameters directly from image use the VGG approach  to embed directly into our mode.skip the latent code. 


% \paragraph{Variational Auto encoders}
% GANs are not the only promising candidate for an neural network architecture for photo realistic image synthesis. It has been shown that "Vector Quantized" \cite{vqvae} \cite{vqvae2} Variational auto encoder are also able to synthesize high quality images of faces. It would be interesting to test the tensor model approach the latent space of these novel Variational Autoencoders.
 


% \begin{itemize}
%     \item We only consider using the same regularisation parameters along all modes. Maybe results would improve by allowing regularization parameters for each mode
% \item Better grip on regularization parameters: Allow for separate numerical  values for each mode.\footnote{And in case of the extended model- each style. }
% \item Time dependency: We can model the time dependency of model parameters by analyzing video.
% \item The VGG shortcut: We should be able to skip the intermediate embedding into $\mathcal{W}$ space by approximating model parameters directly using the VGG approach 
% \item ResNet: We can train an even larger ResNet for StyleGAN2
% \item Try this model on the StyleGAN $\mathcal{Z}$ space.
% \end{itemize}