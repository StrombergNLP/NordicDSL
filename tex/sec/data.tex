
\section{The data set}
The first step when constructing a language classifier is to gather a data set. For this purpose we use a small script using the Wikipedia API for Python.\footnote{\url{https://pypi.org/project/wikipedia/}}

The script helped download the summaries for randomly chosen Wikipedia articles in each of the languages which are saved to as raw text to 6 {\tt .txt} files of about 10MB each.

After the initial cleaning, described in the next section, the data set contains just over 50K sentences in each of the language categories. From this two data sets with exactly 10K and 50K sentences respectively are drawn from the raw data set. In this way the data sets we will work with are balanced, containing the same number of data points in each language category.

Throughout this report we split these data sets, reserving 80\% for the training set and 20\% for the test set we use when evaluating the models.

\subsection{Data Cleaning}
This section describes how the data set is initially cleaned and how sentences are extracted from the raw data.

\paragraph{Extracting Sentences}

The first thing we want to do is to divide the text into sentences.
This is generally a non-trivial thing to do. Our approach is to first split the raw string by line break.
This roughly divides the text into paragraphs with some noise which we filter out later.

We then extract shorter sentences with the sentence tokenizer ({\tt sent\_tokenize}) function from the NLTK\cite{nltk} python package. This does a better job than just splitting by {\tt '.'} due to the fact that abbreviations, which can appear in a legitimate sentence, typically include a period symbol.

\paragraph{Cleaning characters}
The initial data set have a lot of characters that do not belong to the alphabets of the languages we work with. Often the Wikipedia pages for people or places contain the name in the original language. For example a summary might contain Chinese or Russian characters which are arguably irrelevant for the purpose of discriminating between the Nordic languages.

To make the feature extraction simpler, and to reduce the size of the vocabulary, the raw data is converted to lowercase and stripped of all characters with are not part of the standard alphabet of the six languages.

In this way we only accept the following character set
\begin{verbatim}
'abcdefghijklmnopqrstuvwxyzáäåæéíðóöøúýþ '
\end{verbatim}
and replace everything else with white space before continuing to extract the features.
For example the raw sentence
\begin{verbatim}
'Hesbjerg er dannet ved sammenlægning
 af de 2 gårde Store Hesbjerg
 og Lille Hesbjerg i 1822.'
\end{verbatim}
will after this initial cleanup be reduced to
\begin{verbatim}
'hesbjerg er dannet ved sammenlægning
 af de gårde store hesbjerg
 og lille hesbjerg i ',
\end{verbatim}

We thus make the assumption that capital letters, numbers and characters outside this character set do not contribute much information relevant for language classification.
